---
title: "Iris dataset support vector machines clustering and visualization"
output: github_document
theme: readable
author: "Madlen Wilmes"
date: "`r format(Sys.Date())`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(e1071)
library(ggplot2)
```

## Purpose of the script

Use iris data set to explore clustering via support vector machines (SVM) or Support Vector Clustering (SVC). 

## A bit of theory

- useful for labeled data with unknown distribution
- linear SVM is rarely used as real world data is usually more messy than two classes
- linear SVM comprable to linear regression, non-linear SVM comparable to logistic regression
- for linear, binary data, one chooses a classifier line (y = ax + b) and make it equidistant from data points closest to the line of either side. Maximize the margin (m), so data in the test case can be classified even if farther away from training test cluster.


Caution:
- SVM is prone to overfitting

![Linear SVM](LinearSVM.jpg)


Support vector clustering has the following idea: let us transform the points from their space to a higher dimensionality feature space. Find a minimal enclosing sphere in this feature space. In the original space, the sphere becomes a set of disjoing regions. Each region becomes a cluster. (There are also important details like how we choose the feature space, and how we do the transformation, and how we define disjoint regions.)

These disjoint regions are completely different from the regions around K centers in the K-means algorithm. For example, in 2 dimensions, the K-means regions are polygons. The regions of SVC are amoeba-like areas.




## Load Data

```{r load_data}
data(iris)
attach(iris)
```


## Prep data set
```{r}
## split ramdomly into training and test sets
iris[,"train"] <- ifelse(runif(nrow(iris)) < 0.8, 1, 0)
## separate training and test sets by using the flag culumn "train"
trainset <- iris[iris$train == 1,]
testset <- iris[iris$train == 0,]
## remove training flag column
trainset <- subset(trainset, select = -c(train))
testset <- subset(testset, select = -c(train))
```



```{r run SVM}
## build model â€“ linear kernel and C-classification (soft margin) with default cost (C = 1)
## predict Species, using all data points (signified by the dot)
svm_model <- svm(Species ~ ., data = trainset, kernel ="radial")
summary(svm_model)
```

## plot results of training model
```{r plot training model for two traits, keep other two constant}
plot(svm_model, data = trainset, Petal.Width ~ Petal.Length, slice = list(Sepal.Width = 3, Sepal.Length = 4))
```

```{r training set predictions}
pred_train <- predict(svm_model, trainset)
## test set predictions
pred_test <- predict(svm_model, testset)
```

## Confusion matrix and Misclassification error

```{r compare prediction to real labels}
accu_tab <- table(Predicted = pred_test, Real = testset$Species)
accu_tab
## misclassification rate
1 - sum(diag(accu_tab)) / sum(accu_tab)
## classification accuracy
sum(diag(accu_tab)) / sum(accu_tab)
```

Misclasification rate = 1 /30 = 0.033
Classification accuracy = 29 / 30 = 0.967

## Model Tuning (hyperparameter optimization)
```{r tune svm model}
set.seed(42)
## don't run more combinations than your machine can handle (with large data sets)
tune_model <- tune(svm, Species~.,
                   data = trainset, 
                   ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:7))
                   )
## darker regions mean better results
plot(tune_model)
summary(tune_model)
```

## retrieve model with 'best' parameters
```{r new prediction with optimized model parameters}
## grab the optimized model
opti_model <- tune_model$best.model
summary(opti_model)
plot(opti_model, data = trainset, Petal.Width ~ Petal.Length, slice = list(Sepal.Width = 3, Sepal.Length = 4))
```

### re-run on training set
```{r re-run on training set}
pred_train <- predict(opti_model, trainset)
accu_tab <- table(Predicted = pred_train, Real = trainset$Species)
accu_tab
## misclassification rate
1 - sum(diag(accu_tab)) / sum(accu_tab)
## classification accuracy
sum(diag(accu_tab)) / sum(accu_tab)
```

### re-run on test set
```{r re-run on test set}
pred_test <- predict(opti_model, testset)
accu_tab <- table(Predicted = pred_test, Real = testset$Species)
accu_tab
## misclassification rate
1 - sum(diag(accu_tab)) / sum(accu_tab)
## classification accuracy
sum(diag(accu_tab)) / sum(accu_tab)
```
